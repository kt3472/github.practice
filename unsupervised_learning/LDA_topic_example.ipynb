{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "import matplotlib.pyplot\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import regex\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Activity15-Activity17/latimeshealth.txt'\n",
    "df = pandas.read_csv(path, sep=\"|\", header=None)\n",
    "df.columns = [\"id\", \"datetime\", \"tweettext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE:\n",
      "(4171, 3)\n",
      "\n",
      "COLUMN NAMES:\n",
      "Index(['id', 'datetime', 'tweettext'], dtype='object')\n",
      "\n",
      "HEAD:\n",
      "                   id                        datetime  \\\n",
      "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
      "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
      "\n",
      "                                           tweettext  \n",
      "0  Five new running shoes that aim to go the extr...  \n",
      "1  Gym Rat: Disq class at Crunch is intense worko...  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataframe_quick_look(df, nrows):\n",
    "    print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n",
    "    print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n",
    "    print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n",
    "dataframe_quick_look(df, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweettext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576760256031682561</td>\n",
       "      <td>Sat Mar 14 15:02:15 +0000 2015</td>\n",
       "      <td>Five new running shoes that aim to go the extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>576715414811471872</td>\n",
       "      <td>Sat Mar 14 12:04:04 +0000 2015</td>\n",
       "      <td>Gym Rat: Disq class at Crunch is intense worko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576438353555365888</td>\n",
       "      <td>Fri Mar 13 17:43:07 +0000 2015</td>\n",
       "      <td>Noshing through thousands of ideas at Natural ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576438347003908096</td>\n",
       "      <td>Fri Mar 13 17:43:06 +0000 2015</td>\n",
       "      <td>Natural Products Expo also explores beauty, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>576413058177712128</td>\n",
       "      <td>Fri Mar 13 16:02:36 +0000 2015</td>\n",
       "      <td>Free Fitness Weekends in South Bay beach citie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                        datetime  \\\n",
       "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
       "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
       "2  576438353555365888  Fri Mar 13 17:43:07 +0000 2015   \n",
       "3  576438347003908096  Fri Mar 13 17:43:06 +0000 2015   \n",
       "4  576413058177712128  Fri Mar 13 16:02:36 +0000 2015   \n",
       "\n",
       "                                           tweettext  \n",
       "0  Five new running shoes that aim to go the extr...  \n",
       "1  Gym Rat: Disq class at Crunch is intense worko...  \n",
       "2  Noshing through thousands of ideas at Natural ...  \n",
       "3  Natural Products Expo also explores beauty, su...  \n",
       "4  Free Fitness Weekends in South Bay beach citie...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "['Five new running shoes that aim to go the extra mile http://lat.ms/1ELp3wU', 'Gym Rat: Disq class at Crunch is intense workout on pulley system http://lat.ms/1EKOFdr', 'Noshing through thousands of ideas at Natural Products Expo West http://lat.ms/1EHqywg', 'Natural Products Expo also explores beauty, supplements and more http://lat.ms/1EHqyfE', 'Free Fitness Weekends in South Bay beach cities aim to spark activity http://lat.ms/1EH3SMC']\n",
      "\n",
      "LENGTH:\n",
      "4171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = df['tweettext'].tolist()\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_language_identifying(txt):\n",
    "    try:\n",
    "        the_language = langdetect.detect(txt)\n",
    "    except:\n",
    "        the_language = 'none'\n",
    "    return the_language\n",
    "\n",
    "\n",
    "def do_lemmatizing(wrd):\n",
    "    out = nltk.corpus.wordnet.morphy(wrd)\n",
    "    return (wrd if out is None else out)\n",
    "\n",
    "\n",
    "def do_tweet_cleaning(txt):\n",
    "# identify language of tweet\n",
    "# return null if language not english\n",
    "    lg = do_language_identifying(txt)\n",
    "    if lg != 'en':\n",
    "        return None\n",
    "\n",
    "# split the string on whitespace\n",
    "    out = txt.split(' ')\n",
    "\n",
    "# identify screen names\n",
    "# replace with SCREENNAME\n",
    "    out = ['SCREENNAME' if i.startswith('@') else i for i in out]\n",
    "\n",
    "# identify urls\n",
    "# replace with URL\n",
    "    out = ['URL' if bool(regex.search('http[s]?://', i)) else i for i in out]\n",
    "\n",
    "# remove all punctuation\n",
    "    out = [regex.sub('[^\\\\w\\\\s]|\\n', '', i) for i in out]\n",
    "\n",
    "# make all non-keywords lowercase\n",
    "    keys = ['SCREENNAME', 'URL']\n",
    "    out = [i.lower() if i not in keys else i for i in out]\n",
    "\n",
    "# remove keywords\n",
    "    out = [i for i in out if i not in keys]\n",
    "\n",
    "# remove stopwords\n",
    "    list_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_stop_words = [    regex.sub('[^\\\\w\\\\s]', '', i) for i in list_stop_words]\n",
    "\n",
    "    out = [i for i in out if i not in list_stop_words]\n",
    "\n",
    "# lemmatizing\n",
    "    out = [do_lemmatizing(i) for i in out]\n",
    "\n",
    "# keep words 4 or more characters long\n",
    "    out = [i for i in out if len(i) >= 5]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = list(map(do_tweet_cleaning, raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "[['running', 'shoes', 'extra'], ['class', 'crunch', 'intense', 'workout', 'pulley', 'system'], ['thousand', 'natural', 'product'], ['natural', 'product', 'explore', 'beauty', 'supplement'], ['fitness', 'weekend', 'south', 'beach', 'spark', 'activity']]\n",
      "\n",
      "LENGTH:\n",
      "4100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean = list(filter(None.__ne__, clean))\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running shoes extra', 'class crunch intense workout pulley system', 'thousand natural product', 'natural product explore beauty supplement', 'fitness weekend south beach spark activity', 'kayla harrison sacrifice', 'sonic treatment alzheimers disease', 'ultrasound brain restore memory alzheimers needle onlyso farin mouse', 'apple researchkit really medical research', 'warning chantix drink taking might remember']\n"
     ]
    }
   ],
   "source": [
    "clean_sentences = [\" \".join(i) for i in clean]\n",
    "print(clean_sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words = 10\n",
    "number_docs = 10\n",
    "number_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 321)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_df=0.95, \n",
    "    min_df=10, \n",
    "    max_features=number_features\n",
    ")\n",
    "clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n",
    "print(clean_vec1[0])\n",
    "\n",
    "feature_names_vec1 = vectorizer1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number Of Topics  Perplexity Score\n",
      "0                 2        356.367714\n",
      "1                 4        412.411161\n",
      "2                 6        437.894952\n",
      "3                 8        463.372176\n",
      "4                10        487.684619\n",
      "5                12        493.664355\n",
      "6                14        517.631987\n",
      "7                16        516.598648\n",
      "8                18        541.126746\n",
      "9                20        543.837987\n"
     ]
    }
   ],
   "source": [
    "def perplexity_by_ntopic(data, ntopics):\n",
    "    output_dict = {\n",
    "        \"Number Of Topics\": [], \n",
    "        \"Perplexity Score\": []\n",
    "    }\n",
    "    for t in ntopics:\n",
    "        lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "            n_components=t,\n",
    "            learning_method=\"online\",\n",
    "            random_state=0\n",
    "        )\n",
    "        lda.fit(data)\n",
    "        output_dict[\"Number Of Topics\"].append(t)\n",
    "        output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n",
    "    output_df = pandas.DataFrame(output_dict)\n",
    "    index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n",
    "    output_num_topics = output_df.loc[\n",
    "        index_min_perplexity,  # index\n",
    "        \"Number Of Topics\"  # column\n",
    "    ]\n",
    "    return (output_df, output_num_topics)\n",
    "df_perplexity, optimal_num_topics = perplexity_by_ntopic(\n",
    "    clean_vec1, \n",
    "    ntopics=[i for i in range(1, 21) if i % 2 == 0]\n",
    ")\n",
    "print(df_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_components=2,\n",
       "                          random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "    n_components=optimal_num_topics,\n",
    "    learning_method=\"online\",\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(clean_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Topic0                Topic1\n",
      "Word0        (0.062, study)      (0.0382, latfit)\n",
      "Word1      (0.0213, weight)      (0.0347, cancer)\n",
      "Word2       (0.0205, could)      (0.0309, health)\n",
      "Word3       (0.0194, brain)       (0.0265, study)\n",
      "Word4       (0.016, report)      (0.0223, people)\n",
      "Word5       (0.0153, child)     (0.0176, patient)\n",
      "Word6    (0.015, scientist)  (0.0164, researcher)\n",
      "Word7  (0.0144, california)       (0.0163, woman)\n",
      "Word8      (0.0133, common)       (0.0159, death)\n",
      "Word9     (0.0126, medical)      (0.0156, doctor)\n"
     ]
    }
   ],
   "source": [
    "def get_topics(mod, vec, names, docs, ndocs, nwords):\n",
    "    # word to topic matrix\n",
    "    W = mod.components_\n",
    "    W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n",
    "    # topic to document matrix\n",
    "    H = mod.transform(vec)\n",
    "    W_dict = {}\n",
    "    H_dict = {}\n",
    "    for tpc_idx, tpc_val in enumerate(W_norm):\n",
    "        topic = \"Topic{}\".format(tpc_idx)\n",
    "        # formatting w\n",
    "        W_indices = tpc_val.argsort()[::-1][:nwords]\n",
    "        W_names_values = [\n",
    "            (round(tpc_val[j], 4), names[j]) \n",
    "            for j in W_indices\n",
    "        ]\n",
    "        W_dict[topic] = W_names_values\n",
    "        # formatting h\n",
    "        H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n",
    "        H_names_values = [\n",
    "        (round(H[:, tpc_idx][j], 4), docs[j]) \n",
    "            for j in H_indices\n",
    "        ]\n",
    "        H_dict[topic] = H_names_values\n",
    "    W_df = pandas.DataFrame(\n",
    "        W_dict, \n",
    "        index=[\"Word\" + str(i) for i in range(nwords)]\n",
    "    )\n",
    "    H_df = pandas.DataFrame(\n",
    "        H_dict,\n",
    "        index=[\"Doc\" + str(i) for i in range(ndocs)]\n",
    "    )\n",
    "    return (W_df, H_df)\n",
    "\n",
    "W_df, H_df = get_topics(\n",
    "    mod=lda,\n",
    "    vec=clean_vec1,\n",
    "    names=feature_names_vec1,\n",
    "    docs=raw,\n",
    "    ndocs=number_docs, \n",
    "    nwords=number_words\n",
    ")\n",
    "\n",
    "print(W_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.9373, Many animals that suffer from contagi...   \n",
      "Doc1  (0.936, You can't trust your drug dealer, rese...   \n",
      "Doc2  (0.9359, Sun protection is a fashion statement...   \n",
      "Doc3  (0.9356, Some parts of your body \"age\" faster ...   \n",
      "Doc4  (0.9348, Flu shot benefit for adults isn't big...   \n",
      "Doc5  (0.9336, Rotator cuff exercise uses dumbbell h...   \n",
      "Doc6                        (0.9283, @ainavar1 Agreed!)   \n",
      "Doc7  (0.9282, Sentinel chickens form a simple and e...   \n",
      "Doc8  (0.9269, New documentary \"All of Me\" explores ...   \n",
      "Doc9  (0.9268, â€œNo one should assume e-cigarettes ar...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0  (0.9412, Researchers find 2 types of brain atr...  \n",
      "Doc1                (0.938, @mypaleoplan Looks delish!)  \n",
      "Doc2  (0.9372, Is your husband faithful? If so, than...  \n",
      "Doc3  (0.937, Starbucks says it will post calorie co...  \n",
      "Doc4  (0.9356, Food as medicine? What to make of the...  \n",
      "Doc5  (0.9347, Is your kitty's poop a threat to the ...  \n",
      "Doc6  (0.9345, Does your dog know how you're feeling...  \n",
      "Doc7  (0.9337, Pediatricians and family medicine doc...  \n",
      "Doc8  (0.9333, Supplements to boost \"low T\" increase...  \n",
      "Doc9  (0.9325, A sign of progress? ADHD diagnosis ra...  \n"
     ]
    }
   ],
   "source": [
    "print(H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1019802420373571528224244683\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1019802420373571528224244683_data = {\"mdsDat\": {\"x\": [0.24180507960942327, -0.24180507960942327], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [52.60101177576614, 47.398988224233854]}, \"tinfo\": {\"Term\": [\"latfit\", \"cancer\", \"health\", \"people\", \"weight\", \"could\", \"brain\", \"report\", \"woman\", \"scientist\", \"cancer\", \"health\", \"breast\", \"heart\", \"woman\", \"people\", \"latfit\", \"death\", \"doctor\", \"healthcare\", \"researcher\", \"patient\", \"study\", \"brain\", \"could\", \"common\", \"scientist\", \"change\", \"company\", \"medical\", \"research\", \"chronic\", \"report\", \"weight\", \"california\", \"child\", \"study\"], \"Freq\": [236.0, 214.0, 191.0, 138.0, 119.0, 114.0, 108.0, 89.0, 101.0, 83.0, 214.079917962365, 190.44772602351188, 94.02413147736503, 95.16206757324363, 100.5012446185577, 137.4096256450109, 235.3178409493627, 98.10928056191996, 96.3087158877566, 83.3820030458823, 101.33082782600816, 108.4030899256071, 163.49695893717984, 107.77134392396935, 113.75765685399281, 73.94784156026716, 83.16795792041876, 58.290346270210605, 51.87907733722856, 70.12240964066628, 65.49065776926156, 51.76797401990572, 88.93177071092529, 118.49264543161921, 79.93901334908107, 84.80447591626529, 344.63953455568367], \"Total\": [236.0, 214.0, 191.0, 138.0, 119.0, 114.0, 108.0, 89.0, 101.0, 83.0, 214.66650477083826, 191.1478745479408, 94.55580747180196, 95.71382725554038, 101.08814817405286, 138.25996221898865, 236.82320699499536, 98.80165543088121, 96.99616856573888, 84.00798236286673, 103.24953939812619, 125.51072333658003, 508.1364934928635, 108.38770975742732, 114.49669825760785, 74.47958564135114, 83.77199165830874, 58.89283198715618, 52.41666607028812, 70.85172829606363, 66.17651430997132, 52.319137394788044, 89.89009827450577, 119.86958594912302, 80.81188610988222, 86.37222346853945, 508.1364934928635], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.3608, -3.4778, -4.1836, -4.1715, -4.117, -3.8042, -3.2662, -4.141, -4.1596, -4.3037, -4.1087, -4.0413, -3.6303, -3.943, -3.8889, -4.3196, -4.2021, -4.5576, -4.6741, -4.3728, -4.4411, -4.6762, -4.1351, -3.8481, -4.2417, -4.1826, -2.7805], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6397, 0.6388, 0.6368, 0.6367, 0.6366, 0.6363, 0.6361, 0.6354, 0.6353, 0.635, 0.6237, 0.4959, -0.4915, 0.7409, 0.7401, 0.7394, 0.7393, 0.7363, 0.7363, 0.7362, 0.7362, 0.736, 0.7359, 0.735, 0.7357, 0.7283, 0.3583]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.009226138297764655, 0.9964229361585828, 0.9941219107883172, 0.010575765008386353, 0.012374417281144403, 0.9899533824915523, 0.9968951617694164, 0.004658388606399142, 0.016979995124331737, 0.9848397172112406, 0.023155592384726413, 0.9841126763508726, 0.019113464972754284, 0.9939001785832227, 0.013426497897227815, 0.9935608443948584, 0.019077901647904315, 0.9920508856910244, 0.008733876305761105, 0.995661898856766, 0.991886214584309, 0.01012128790392152, 0.989729815306429, 0.010309685576108635, 0.9939948348855278, 0.0052315517625554095, 0.98800135017512, 0.011903630725001446, 0.9925420675778164, 0.010447811237661225, 0.9923014006180826, 0.008445118303132618, 0.014113981748213161, 0.9879787223749212, 0.8604842449228676, 0.13544659410822915, 0.9908870059070825, 0.007232751867934909, 0.01112469581406182, 0.9900979274515019, 0.015111101127448207, 0.9822215732841334, 0.9782125962862454, 0.01937054646111377, 0.01193716396380815, 0.9907846089960765, 0.3207799520155685, 0.678951432180191, 0.008342399717843658, 0.9844031667055516, 0.9991280068371508, 0.009892356503338126], \"Term\": [\"brain\", \"brain\", \"breast\", \"breast\", \"california\", \"california\", \"cancer\", \"cancer\", \"change\", \"change\", \"child\", \"child\", \"chronic\", \"chronic\", \"common\", \"common\", \"company\", \"company\", \"could\", \"could\", \"death\", \"death\", \"doctor\", \"doctor\", \"health\", \"health\", \"healthcare\", \"healthcare\", \"heart\", \"heart\", \"latfit\", \"latfit\", \"medical\", \"medical\", \"patient\", \"patient\", \"people\", \"people\", \"report\", \"report\", \"research\", \"research\", \"researcher\", \"researcher\", \"scientist\", \"scientist\", \"study\", \"study\", \"weight\", \"weight\", \"woman\", \"woman\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1019802420373571528224244683\", ldavis_el1019802420373571528224244683_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1019802420373571528224244683\", ldavis_el1019802420373571528224244683_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1019802420373571528224244683\", ldavis_el1019802420373571528224244683_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n",
    "pyLDAvis.display(lda_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
